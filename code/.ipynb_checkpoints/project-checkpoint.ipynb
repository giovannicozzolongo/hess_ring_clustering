{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Explorer: Unsupervised Muon Tagging\n",
    "\n",
    "This notebook scans and analyzes all files in the `unsupervided_muon_tagging` project directory to understand what data, code, and results we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = Path('../')\n",
    "print(f\"Exploring: {BASE_DIR.resolve()}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Directory Structure\n",
    "\n",
    "Let's first understand the overall structure of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory_tree(base_path, exclude_patterns=None, max_depth=3):\n",
    "    \"\"\"\n",
    "    Recursively scan directory and return file statistics.\n",
    "    \"\"\"\n",
    "    if exclude_patterns is None:\n",
    "        exclude_patterns = ['.git', '__pycache__', '.ipynb_checkpoints', 'animal_images']\n",
    "    \n",
    "    file_stats = defaultdict(lambda: {'count': 0, 'total_size': 0, 'files': []})\n",
    "    dir_structure = {}\n",
    "    \n",
    "    def should_exclude(path_str):\n",
    "        return any(pattern in path_str for pattern in exclude_patterns)\n",
    "    \n",
    "    def scan_recursive(path, depth=0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            items = sorted(path.iterdir())\n",
    "        except PermissionError:\n",
    "            return\n",
    "        \n",
    "        for item in items:\n",
    "            if should_exclude(str(item)):\n",
    "                continue\n",
    "            \n",
    "            if item.is_file():\n",
    "                size = item.stat().st_size\n",
    "                ext = item.suffix or 'no_extension'\n",
    "                \n",
    "                file_stats[ext]['count'] += 1\n",
    "                file_stats[ext]['total_size'] += size\n",
    "                file_stats[ext]['files'].append({\n",
    "                    'path': str(item.relative_to(base_path)),\n",
    "                    'size': size,\n",
    "                    'name': item.name\n",
    "                })\n",
    "            \n",
    "            elif item.is_dir():\n",
    "                scan_recursive(item, depth + 1)\n",
    "    \n",
    "    scan_recursive(base_path)\n",
    "    return dict(file_stats)\n",
    "\n",
    "# Scan the project\n",
    "file_stats = scan_directory_tree(BASE_DIR)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FILE TYPE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Extension':<15} {'Count':<10} {'Total Size':<20} {'Avg Size'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for ext, stats in sorted(file_stats.items(), key=lambda x: x[1]['total_size'], reverse=True):\n",
    "    total_mb = stats['total_size'] / (1024 * 1024)\n",
    "    avg_mb = total_mb / stats['count']\n",
    "    print(f\"{ext:<15} {stats['count']:<10} {total_mb:<20.2f} MB {avg_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Directory Analysis\n",
    "\n",
    "Explore what's in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = BASE_DIR / 'data'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA DIRECTORY CONTENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if data_dir.exists():\n",
    "    for item in sorted(data_dir.iterdir()):\n",
    "        if item.name == 'animal_images':\n",
    "            continue\n",
    "        \n",
    "        if item.is_dir():\n",
    "            n_files = len(list(item.glob('*')))\n",
    "            total_size = sum(f.stat().st_size for f in item.glob('*') if f.is_file())\n",
    "            print(f\"\\nüìÅ {item.name}/\")\n",
    "            print(f\"   Files: {n_files}\")\n",
    "            print(f\"   Total size: {total_size / (1024**3):.2f} GB\")\n",
    "            \n",
    "            # Show first few files\n",
    "            files = sorted(item.glob('*'))[:5]\n",
    "            for f in files:\n",
    "                if f.is_file():\n",
    "                    print(f\"   - {f.name} ({f.stat().st_size / (1024**2):.1f} MB)\")\n",
    "            if n_files > 5:\n",
    "                print(f\"   ... and {n_files - 5} more files\")\n",
    "        \n",
    "        elif item.is_file():\n",
    "            size_mb = item.stat().st_size / (1024**2)\n",
    "            print(f\"\\nüìÑ {item.name} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"Data directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze HDF5 Files\n",
    "\n",
    "Look inside the FlashCam HDF5 files to understand their structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flashcam_dir = BASE_DIR / 'data' / 'flashcam'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HDF5 FILE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if flashcam_dir.exists():\n",
    "    h5_files = sorted(flashcam_dir.glob('*.h5'))\n",
    "    print(f\"\\nFound {len(h5_files)} HDF5 files\")\n",
    "    \n",
    "    if h5_files:\n",
    "        # Analyze first file in detail\n",
    "        sample_file = h5_files[0]\n",
    "        print(f\"\\nAnalyzing sample file: {sample_file.name}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        with tables.open_file(sample_file, 'r') as f:\n",
    "            print(\"\\nDatasets found:\")\n",
    "            for node in f.root:\n",
    "                if isinstance(node, tables.Array):\n",
    "                    print(f\"  - {node.name}:\")\n",
    "                    print(f\"      Shape: {node.shape}\")\n",
    "                    print(f\"      Dtype: {node.dtype}\")\n",
    "                    print(f\"      Size: {node.size_in_memory / (1024**2):.2f} MB\")\n",
    "                    \n",
    "                    # Sample statistics\n",
    "                    if node.name == 'images':\n",
    "                        sample_img = node[0]\n",
    "                        print(f\"      Sample image stats:\")\n",
    "                        print(f\"        Min: {np.nanmin(sample_img):.6f}\")\n",
    "                        print(f\"        Max: {np.nanmax(sample_img):.6f}\")\n",
    "                        print(f\"        Mean: {np.nanmean(sample_img):.6f}\")\n",
    "                        print(f\"        NaN pixels: {np.isnan(sample_img).sum()} / {sample_img.size} \"\n",
    "                              f\"({np.isnan(sample_img).sum() / sample_img.size * 100:.1f}%)\")\n",
    "        \n",
    "        # Statistics across all files\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STATISTICS ACROSS ALL HDF5 FILES\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        total_images = 0\n",
    "        file_sizes = []\n",
    "        \n",
    "        for h5_file in h5_files[:10]:  # Sample first 10 files\n",
    "            with tables.open_file(h5_file, 'r') as f:\n",
    "                if 'images' in f.root:\n",
    "                    total_images += len(f.root.images)\n",
    "                    file_sizes.append(h5_file.stat().st_size / (1024**2))\n",
    "        \n",
    "        print(f\"\\nSampled {len(file_sizes)} files:\")\n",
    "        print(f\"  Total images in sample: {total_images}\")\n",
    "        print(f\"  Average images per file: {total_images / len(file_sizes):.0f}\")\n",
    "        print(f\"  Average file size: {np.mean(file_sizes):.2f} MB\")\n",
    "        print(f\"  Total file size (sample): {np.sum(file_sizes):.2f} MB\")\n",
    "        \n",
    "        # Estimate total\n",
    "        estimated_total = (total_images / len(file_sizes)) * len(h5_files)\n",
    "        print(f\"\\n  Estimated total images in all {len(h5_files)} files: {estimated_total:.0f}\")\n",
    "else:\n",
    "    print(\"FlashCam directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Images\n",
    "\n",
    "Show some random images from the HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flashcam_dir.exists() and h5_files:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SAMPLE IMAGES FROM HDF5 FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load images from first file\n",
    "    with tables.open_file(h5_files[0], 'r') as f:\n",
    "        images = f.root.images[:12]  # First 12 images\n",
    "    \n",
    "    # Replace NaN with 0 for visualization\n",
    "    images = np.nan_to_num(images, nan=0.0)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        axes[i].imshow(img, cmap='viridis')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Event {i}\\nMax: {img.max():.1f}')\n",
    "    \n",
    "    plt.suptitle(f'Sample images from {h5_files[0].name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nImage statistics (first 12 events):\")\n",
    "    print(f\"  Non-zero pixels per image: {(images > 0).sum(axis=(1,2)).mean():.1f} ¬± {(images > 0).sum(axis=(1,2)).std():.1f}\")\n",
    "    print(f\"  Mean intensity: {images.mean():.4f}\")\n",
    "    print(f\"  Max intensity: {images.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code Directory Analysis\n",
    "\n",
    "Examine notebooks and scripts in the code directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = BASE_DIR / 'code'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE DIRECTORY CONTENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if code_dir.exists():\n",
    "    notebooks = sorted(code_dir.glob('*.ipynb'))\n",
    "    scripts = sorted(code_dir.glob('*.py'))\n",
    "    \n",
    "    print(f\"\\nFound {len(notebooks)} notebooks:\")\n",
    "    for nb in notebooks:\n",
    "        size_mb = nb.stat().st_size / (1024**2)\n",
    "        print(f\"  üìì {nb.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Try to read notebook metadata\n",
    "        try:\n",
    "            with open(nb, 'r') as f:\n",
    "                nb_data = json.load(f)\n",
    "                n_cells = len(nb_data.get('cells', []))\n",
    "                print(f\"      Cells: {n_cells}\")\n",
    "                \n",
    "                # Count code vs markdown\n",
    "                code_cells = sum(1 for c in nb_data.get('cells', []) if c.get('cell_type') == 'code')\n",
    "                markdown_cells = sum(1 for c in nb_data.get('cells', []) if c.get('cell_type') == 'markdown')\n",
    "                print(f\"      Code: {code_cells}, Markdown: {markdown_cells}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if scripts:\n",
    "        print(f\"\\nFound {len(scripts)} Python scripts:\")\n",
    "        for script in scripts:\n",
    "            size_kb = script.stat().st_size / 1024\n",
    "            print(f\"  üêç {script.name} ({size_kb:.1f} KB)\")\n",
    "else:\n",
    "    print(\"Code directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Directory Analysis\n",
    "\n",
    "Check what results have been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = BASE_DIR / 'results'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESULTS DIRECTORY CONTENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if results_dir.exists():\n",
    "    result_files = sorted(results_dir.glob('*'))\n",
    "    \n",
    "    if result_files:\n",
    "        print(f\"\\nFound {len(result_files)} result files:\")\n",
    "        for rf in result_files:\n",
    "            if rf.is_file():\n",
    "                size_mb = rf.stat().st_size / (1024**2)\n",
    "                print(f\"\\n  üìä {rf.name} ({size_mb:.2f} MB)\")\n",
    "                \n",
    "                # Try to load and analyze\n",
    "                if rf.suffix == '.npz':\n",
    "                    data = np.load(rf)\n",
    "                    print(f\"      Arrays: {list(data.keys())}\")\n",
    "                    for key in data.keys():\n",
    "                        arr = data[key]\n",
    "                        print(f\"        {key}: shape={arr.shape}, dtype={arr.dtype}\")\n",
    "    else:\n",
    "        print(\"\\nResults directory is empty\")\n",
    "else:\n",
    "    print(\"\\nResults directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Other Files\n",
    "\n",
    "Check for any other relevant files (config files, documentation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"OTHER FILES IN PROJECT ROOT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for item in sorted(BASE_DIR.iterdir()):\n",
    "    if item.name in ['data', 'code', 'results']:\n",
    "        continue\n",
    "    \n",
    "    if item.is_file():\n",
    "        size = item.stat().st_size\n",
    "        if size < 1024:\n",
    "            size_str = f\"{size} B\"\n",
    "        elif size < 1024**2:\n",
    "            size_str = f\"{size/1024:.1f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{size/(1024**2):.1f} MB\"\n",
    "        \n",
    "        print(f\"  {item.name} ({size_str})\")\n",
    "        \n",
    "        # Show first few lines if it's a text file\n",
    "        if item.suffix in ['.txt', '.md', '.py', '.sh', '.dat']:\n",
    "            try:\n",
    "                with open(item, 'r') as f:\n",
    "                    lines = f.readlines()[:3]\n",
    "                    for line in lines:\n",
    "                        print(f\"    {line.rstrip()}\")\n",
    "                if len(lines) > 3:\n",
    "                    print(\"    ...\")\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count files by type\n",
    "h5_count = len(list(flashcam_dir.glob('*.h5'))) if flashcam_dir.exists() else 0\n",
    "nb_count = len(list(code_dir.glob('*.ipynb'))) if code_dir.exists() else 0\n",
    "result_count = len(list(results_dir.glob('*'))) if results_dir.exists() else 0\n",
    "\n",
    "print(f\"\\nüìÅ Project: unsupervided_muon_tagging\")\n",
    "print(f\"   Location: {BASE_DIR.resolve()}\")\n",
    "print(f\"\\nüìä Data:\")\n",
    "print(f\"   - HDF5 files: {h5_count}\")\n",
    "if h5_count > 0:\n",
    "    total_size = sum(f.stat().st_size for f in flashcam_dir.glob('*.h5')) / (1024**3)\n",
    "    print(f\"   - Total size: {total_size:.2f} GB\")\n",
    "    print(f\"   - Estimated total events: {estimated_total:.0f}\" if 'estimated_total' in locals() else \"\")\n",
    "\n",
    "print(f\"\\nüíª Code:\")\n",
    "print(f\"   - Notebooks: {nb_count}\")\n",
    "\n",
    "print(f\"\\nüìà Results:\")\n",
    "print(f\"   - Saved files: {result_count}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Project appears to be set up for:\")\n",
    "print(f\"   1. Processing FlashCam telescope images\")\n",
    "print(f\"   2. Applying unsupervised learning (clustering)\")\n",
    "print(f\"   3. Identifying muon ring signatures\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
