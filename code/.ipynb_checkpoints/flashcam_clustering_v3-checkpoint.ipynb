{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashCam clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.applications import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flashcam_data(data_dir, max_images=None, test_split=0.2):\n",
    "    # reads telescope camera images from h5 files\n",
    "    all_images = []\n",
    "    all_event_nrs = []\n",
    "    \n",
    "    h5_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.h5')])\n",
    "    print(f\"found {len(h5_files)} files\")\n",
    "    \n",
    "    for h5_file in h5_files:\n",
    "        filepath = os.path.join(data_dir, h5_file)\n",
    "        \n",
    "        with tables.open_file(filepath, 'r') as f:\n",
    "            images = f.root.images[:]\n",
    "            event_nrs = f.root.event_nr[:]\n",
    "            all_images.append(images)\n",
    "            all_event_nrs.append(event_nrs)\n",
    "            print(f\"{h5_file}: {len(images)} images\")\n",
    "        \n",
    "        if max_images and len(np.concatenate(all_images)) >= max_images:\n",
    "            break\n",
    "    \n",
    "    all_images = np.concatenate(all_images)\n",
    "    all_event_nrs = np.concatenate(all_event_nrs)\n",
    "    \n",
    "    # take random subset if needed\n",
    "    if max_images and len(all_images) > max_images:\n",
    "        indices = np.random.choice(len(all_images), max_images, replace=False)\n",
    "        all_images = all_images[indices]\n",
    "        all_event_nrs = all_event_nrs[indices]\n",
    "    \n",
    "    print(f\"\\ntotal {len(all_images)} images, shape {all_images[0].shape}\")\n",
    "    \n",
    "    # split into train and test sets\n",
    "    x_train, x_test, evt_train, evt_test = train_test_split(\n",
    "        all_images, all_event_nrs, test_size=test_split, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"train {len(x_train)}, test {len(x_test)}\")\n",
    "    return x_train, x_test, evt_train, evt_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/flashcam'\n",
    "\n",
    "# using 10k images for now, faster to test\n",
    "x_train, x_test, evt_train, evt_test = load_flashcam_data(DATA_DIR, max_images=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the data\n",
    "fig, axes = plt.subplots(2, 6, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i], cmap='viridis')\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check basic statistics\n",
    "print(f\"min {x_train.min():.2f}, max {x_train.max():.2f}\")\n",
    "print(f\"mean {x_train.mean():.2f}, std {x_train.std():.2f}\")\n",
    "\n",
    "# distribution of pixel values\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(x_train.flatten(), bins=100)\n",
    "plt.xlabel('pixel value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction with VGG19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_vgg19(images, batch_size=32):\n",
    "    # use pretrained CNN to extract features instead of raw pixels\n",
    "    # should give better representation for clustering\n",
    "    \n",
    "    # vgg19 expects RGB images with 3 channels\n",
    "    images_rgb = np.repeat(images[..., np.newaxis], 3, axis=-1)\n",
    "    \n",
    "    # resize to 224x224 which is the vgg19 input size\n",
    "    from tensorflow.image import resize\n",
    "    images_resized = []\n",
    "    for img in tqdm(images_rgb, desc=\"resizing\"):\n",
    "        resized = resize(img, (224, 224))\n",
    "        images_resized.append(resized.numpy())\n",
    "    images_resized = np.array(images_resized)\n",
    "    \n",
    "    # preprocess for vgg19, normalizes using imagenet mean and std\n",
    "    images_preprocessed = preprocess_input(images_resized)\n",
    "    \n",
    "    # load vgg19 pretrained on imagenet\n",
    "    # include_top=False removes the classification layer\n",
    "    # pooling=avg gives us a fixed size feature vector\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features = base_model.predict(images_preprocessed, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from train and test\n",
    "features_train = extract_features_vgg19(x_train)\n",
    "features_test = extract_features_vgg19(x_test)\n",
    "\n",
    "print(f\"features shape: {features_train.shape}\")  # each image becomes 512-dim vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512 dimensions is still a lot for clustering\n",
    "# first check how many components we actually need\n",
    "pca_full = PCA()\n",
    "pca_full.fit(features_train)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('n components')\n",
    "plt.ylabel('cumulative variance')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(pca_full.explained_variance_ratio_[:50])\n",
    "plt.xlabel('component')\n",
    "plt.ylabel('variance ratio')\n",
    "plt.show()\n",
    "\n",
    "# see how many components capture most variance\n",
    "n_90 = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) > 0.9) + 1\n",
    "n_95 = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) > 0.95) + 1\n",
    "print(f\"90% variance: {n_90} components\")\n",
    "print(f\"95% variance: {n_95} components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 50 components, good balance between dimensionality and information\n",
    "N_COMPONENTS = 50\n",
    "\n",
    "pca = PCA(n_components=N_COMPONENTS)\n",
    "features_train_pca = pca.fit_transform(features_train)\n",
    "features_test_pca = pca.transform(features_test)\n",
    "\n",
    "print(f\"reduced to {features_train_pca.shape}\")\n",
    "print(f\"variance explained: {pca.explained_variance_ratio_.sum():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project to 2d just for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "features_2d = pca_2d.fit_transform(features_train_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1], alpha=0.3, s=10)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different k values and plot inertia\n",
    "# looking for elbow in the curve\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(features_train_pca)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    print(f\"k={k}: inertia={kmeans.inertia_:.1f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertias, 'o-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('inertia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow looks around k=4, try that\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels_k4 = kmeans_4.fit_predict(features_train_pca)\n",
    "\n",
    "# check how many events in each cluster\n",
    "for i in range(4):\n",
    "    print(f\"cluster {i}: {(labels_k4==i).sum()} events\")\n",
    "\n",
    "# visualize clusters in 2d space\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(4):\n",
    "    mask = labels_k4 == i\n",
    "    plt.scatter(features_2d[mask, 0], features_2d[mask, 1], \n",
    "                alpha=0.5, s=10, label=f'cluster {i}')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at actual images from each cluster to see if grouping makes sense\n",
    "fig, axes = plt.subplots(4, 6, figsize=(12, 8))\n",
    "\n",
    "for cluster_id in range(4):\n",
    "    cluster_idx = np.where(labels_k4 == cluster_id)[0]\n",
    "    sample_idx = np.random.choice(cluster_idx, 6, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        axes[cluster_id, i].imshow(x_train[idx], cmap='viridis')\n",
    "        axes[cluster_id, i].axis('off')\n",
    "    \n",
    "    axes[cluster_id, 0].set_ylabel(f'Cluster {cluster_id}', \n",
    "                                    rotation=0, labelpad=30, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM is like soft k-means, allows probabilistic cluster assignments\n",
    "# use BIC and AIC to choose number of components\n",
    "n_components_range = range(2, 11)\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "\n",
    "for n in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "    gmm.fit(features_train_pca)\n",
    "    bic_scores.append(gmm.bic(features_train_pca))\n",
    "    aic_scores.append(gmm.aic(features_train_pca))\n",
    "    print(f\"n={n}: BIC={bic_scores[-1]:.1f}, AIC={aic_scores[-1]:.1f}\")\n",
    "\n",
    "# lower BIC/AIC is better\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(n_components_range, bic_scores, 'o-', label='BIC')\n",
    "plt.plot(n_components_range, aic_scores, 's-', label='AIC')\n",
    "plt.xlabel('n components')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try n=4 for comparison with k-means\n",
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "labels_gmm = gmm.fit_predict(features_train_pca)\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"component {i}: {(labels_gmm==i).sum()} events\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(4):\n",
    "    mask = labels_gmm == i\n",
    "    plt.scatter(features_2d[mask, 0], features_2d[mask, 1], \n",
    "                alpha=0.5, s=10, label=f'component {i}')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check images from GMM components\n",
    "fig, axes = plt.subplots(4, 6, figsize=(12, 8))\n",
    "\n",
    "for comp_id in range(4):\n",
    "    comp_idx = np.where(labels_gmm == comp_id)[0]\n",
    "    sample_idx = np.random.choice(comp_idx, 6, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        axes[comp_id, i].imshow(x_train[idx], cmap='viridis')\n",
    "        axes[comp_id, i].axis('off')\n",
    "    \n",
    "    axes[comp_id, 0].set_ylabel(f'Component {comp_id}', \n",
    "                                 rotation=0, labelpad=30, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# DBSCAN is density-based, can find arbitrary shaped clusters\n",
    "# also identifies noise points that don't belong to any cluster\n",
    "# eps is max distance between points in same cluster\n",
    "# min_samples is min points needed to form dense region\n",
    "\n",
    "eps_vals = [2, 3, 5]\n",
    "min_samples_vals = [5, 10, 20]\n",
    "\n",
    "results = []\n",
    "\n",
    "for eps in eps_vals:\n",
    "    for min_samp in min_samples_vals:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samp)\n",
    "        labels = dbscan.fit_predict(features_train_pca)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        print(f\"eps={eps}, min_samples={min_samp}: \"\n",
    "              f\"{n_clusters} clusters, {n_noise} noise ({n_noise/len(labels)*100:.1f}%)\")\n",
    "        \n",
    "        results.append({\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samp,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'labels': labels\n",
    "        })\n",
    "\n",
    "# want reasonable number of clusters and not too much noise\n",
    "good_results = [r for r in results \n",
    "                if 3 <= r['n_clusters'] <= 6 and r['n_noise']/len(labels_k4) < 0.2]\n",
    "\n",
    "if not good_results:\n",
    "    best = min(results, key=lambda x: x['n_noise'])\n",
    "else:\n",
    "    best = max(good_results, key=lambda x: x['n_clusters'])\n",
    "\n",
    "print(f\"\\nusing eps={best['eps']}, min_samples={best['min_samples']}\")\n",
    "dbscan_labels = best['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dbscan results\n",
    "n_clusters = best['n_clusters']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_clusters):\n",
    "    mask = dbscan_labels == i\n",
    "    plt.scatter(features_2d[mask, 0], features_2d[mask, 1],\n",
    "                alpha=0.5, s=10, label=f'cluster {i}')\n",
    "\n",
    "# plot noise points separately\n",
    "if best['n_noise'] > 0:\n",
    "    mask = dbscan_labels == -1\n",
    "    plt.scatter(features_2d[mask, 0], features_2d[mask, 1],\n",
    "                c='gray', alpha=0.3, s=5, marker='x', label='noise')\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample images from dbscan clusters\n",
    "fig, axes = plt.subplots(n_clusters, 6, figsize=(12, 2*n_clusters))\n",
    "if n_clusters == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_idx = np.where(dbscan_labels == cluster_id)[0]\n",
    "    n_samples = min(6, len(cluster_idx))\n",
    "    sample_idx = np.random.choice(cluster_idx, n_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        axes[cluster_id, i].imshow(x_train[idx], cmap='viridis')\n",
    "        axes[cluster_id, i].axis('off')\n",
    "    \n",
    "    for i in range(n_samples, 6):\n",
    "        axes[cluster_id, i].axis('off')\n",
    "    \n",
    "    axes[cluster_id, 0].set_ylabel(f'Cluster {cluster_id}', \n",
    "                                    rotation=0, labelpad=30, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muon_tagging",
   "language": "python",
   "name": "muon_tagging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}